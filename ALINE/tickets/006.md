### ğŸ§ª `006_training_loop_and_losses.md`

# ğŸ§ª Ticket 006 â€“ Training Loop, Losses, and Logging

**Date:** 2025-11-15
**Owner:** ML / Modelling
**Status:** ğŸ”§ To Do
**Goal:** Implement training loop for simplified ALINE with (i) posterior fit loss and (ii) proxy policy objective using uncertainty reduction.

---

## ğŸ¯ Objective

Train on hourly sequences (24-hour windows) to predict next-day migraine probability and produce policy scores that prioritize uncertain/impactful hours.

---

## ğŸ“‚ Inputs

| File                                | Description               |
| ----------------------------------- | ------------------------- |
| `models/aline.py`                   | From Ticket 005           |
| `data/synthetic_migraine_train.csv` | Simulated data            |
| `scripts/train_aline.py`            | Trainer script            |
| `configs/train.yaml`                | LR, batch size, scheduler |

---

## ğŸ§© Tasks

### 1) Posterior Fit Loss (Simplified)

* Treat simulator latent (Z_t) as target (available in synthetic data).
* Minimize ( \mathcal{L}_{post} = \text{MSE}(\mu_z, Z_t) + \lambda \cdot \text{MSE}(\sigma_z, \sigma^*) )
  (where (\sigma^*) is a small constant or simulator-provided noise).

### 2) Policy Proxy Loss

* Compute predictive migraine prob (\hat{p}_{t+1}) via `w,b` on (\mu_z).
* Reward = **uncertainty reduction**: higher when entropy (H(\hat{p})) is high and variance in (Z) is high.
* Use detached reward as weights for cross-entropy on selected top-k policy indices to avoid full RL for now.

```python
# pseudo inside training step
posterior, policy_scores = model(batch_x)          # [B,T], [B,T]
mu_z, sigma_z = posterior.mean, posterior.stddev
p = torch.sigmoid((mu_z @ w) + b)                  # [B,T,z] @ [z] -> [B,T]
entropy = -(p*torch.log(p+1e-6) + (1-p)*torch.log(1-p+1e-6))
uncert = entropy + 0.1*sigma_z.mean(-1)            # [B,T]
topk = policy_scores.topk(k=3, dim=1).indices      # pick 3 hours
policy_loss = -uncert.gather(1, topk).mean()       # maximize uncert -> minimize negative
```

### 3) Overall Loss

`loss = L_post + Î± * policy_loss + Î² * CE(y_next, p_next)` where `y_next` is next-day migraine label.

### 4) Logging

* Use `wandb` or CSV logger for: `loss`, `L_post`, `policy_loss`, `auc`, `brier`.

---

## ğŸ§  Integration

* Feeds metrics to Ticket 009 evaluation.
* Produces model artifacts for Ticket 010 FastAPI.

---

## âœ… Deliverables

* [ ] `scripts/train_aline.py`
* [ ] `configs/train.yaml`
* [ ] `runs/` with logs + best checkpoint

---

> *â€œIf you canâ€™t train it, you canâ€™t ship it.â€*

---
