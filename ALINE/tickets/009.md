### ğŸ§ª `009_evaluation_metrics_and_baselines.md`

# ğŸ§ª Ticket 009 â€“ Metrics, Baselines, and QA

**Date:** 2025-11-15
**Owner:** ML / Modelling
**Status:** ğŸ”§ To Do
**Goal:** Define + compute key metrics and compare against simple baselines.

---

## ğŸ¯ Objective

Metrics:

* **ROC-AUC** for next-day migraine prediction.
* **Brier score** (calibration).
* **Uncertainty reduction** vs random querying (entropy drop).

Baselines:

* **Random hours** policy.
* **Fixed schedule** (e.g., hours {08, 12, 20}).
* **No-policy** (use all hours equally).

---

## ğŸ“‚ Inputs

| File                       | Description         |
| -------------------------- | ------------------- |
| `scripts/eval.py`          | Metric computations |
| `data/simulated_eval.csv`  | Evaluation set      |
| `runs/checkpoints/best.pt` | Model               |

---

## ğŸ§© Tasks

1. Implement `eval.py` functions for AUC, Brier, PR-AUC.
2. Policy simulation to measure mean entropy reduction vs baselines.
3. Save JSON summary + plots.

---

## ğŸ§  Integration

* Gate to promotion of checkpoints for FastAPI.
* Numbers for README & demo.

---

## âœ… Deliverables

* [ ] `scripts/eval.py`
* [ ] `reports/eval_summary.json`
* [ ] `reports/plots/*.png`

---

> *â€œWhat gets measured gets improved.â€*

---
