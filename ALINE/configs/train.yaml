# ALINE Training Configuration - Ticket 006
# Configuration for training the simplified ALINE model

# Data paths
data:
  train: data/synthetic_migraine_train.csv
  val: data/synthetic_migraine_val.csv

# Limit sequences for faster testing (set to null or remove for full dataset)
# With 2.9M rows, full dataset creates ~2.9M sequences which takes forever to load
max_sequences_per_dataset: 100000  # Start with 10k sequences for testing
  
# Model configuration
model:
  in_dim: 35  # 20 base + 4 temporal (Ticket 020) + 11 new (Ticket 025)
  z_dim: 4  # Latent dimensions
  d_model: 64
  nhead: 4
  nlayers: 3

# Training hyperparameters
training:
  batch_size: 32
  learning_rate: 0.0001  # Reduced from 0.001 to prevent exploding gradients
  num_epochs: 500  # Reduced for testing
  sequence_length: 24  # 24-hour windows
  
  # Loss weights (reduced to prevent explosion)
  alpha_policy: 0.01  # Weight for policy loss (reduced from 0.1)
  beta_migraine: 0.5  # Weight for migraine prediction loss (reduced from 1.0)
  lambda_sigma: 0.1  # Weight for sigma regularization in posterior loss (reduced from 0.5)
  
  # Gradient clipping
  max_grad_norm: 0.5  # Clip gradients to prevent explosion
  
  # Optimizer
  optimizer: adam
  weight_decay: 0.0001
  
  # Scheduler
  scheduler: cosine
  warmup_epochs: 10  # Increased warmup for stability
  
  # Early stopping
  patience: 10
  min_delta: 0.0001

# Checkpointing
checkpoint:
  save_dir: runs/checkpoints
  save_best: true
  save_every: 5  # Save every N epochs
  
# Logging
logging:
  log_dir: runs/logs
  log_interval: 10  # Log every N batches
  csv_log: runs/logs/training.csv
  
# Device
device: auto  # 'cuda', 'cpu', or 'auto'

# Random seed
seed: 42
